{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KoW38RfQn0JsssA3pgek0BtfqTzz9U6J",
      "authorship_tag": "ABX9TyP+kPCnEnqhAEdciWgO7qQf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshil2424/textutilsharshil/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import datetime\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "mEgCDDqGSDyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfXdUfMxOUC6"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-17-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "metadata": {
        "id": "Y0twtKUeOms6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!java --enable-preview -jar /content/drive/MyDrive/trained_model/jadbio-model-exe.jar"
      ],
      "metadata": {
        "id": "7Tgrxnh5PQ3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = [4,12,30,33,36,42,49,50,51,53,70,112,155,158,164,206,261,307,312,357,378,406]"
      ],
      "metadata": {
        "id": "pXO68mCJS8eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getdata():\n",
        "#------------------------GETTING HTML TABLE DATA---------------------------\n",
        "  url = f'https://ltp.investingdaddy.com/detailed-options-chain.php'\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "      html_source = response.text\n",
        "      #print(html_source)\n",
        "  else:\n",
        "      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "  #------------------------FILTERING TABLE DATA-------------------------------\n",
        "  soup = BeautifulSoup(html_source, 'html.parser')\n",
        "  tables = soup.find_all('table')\n",
        "  ##price = soup.find('label', {'id':'future_val'})\n",
        "  ##price = price.text\n",
        "  ##prices.append(price)\n",
        "  ##return\n",
        "\n",
        "  if len(tables) >= 2:\n",
        "      second_table = tables[1]\n",
        "  else:\n",
        "      print(\"There are not enough tables in the HTML source.\")\n",
        "\n",
        "  #-----------------------CONVERTING HTML TABLE DATA TO CSV--------------------\n",
        "  html_content = \"<html>\"+str(second_table)+\"</html>\"\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "  table = soup.find('table', {'id': 'tech-companies-1'})\n",
        "  table_data = []\n",
        "  rows = table.find_all('tr')\n",
        "  for row in rows:\n",
        "      row_data = []\n",
        "      cols = row.find_all(['th', 'td'])\n",
        "      for col in cols:\n",
        "          row_data.append(col.get_text(strip=True))\n",
        "      table_data.append(row_data)\n",
        "\n",
        "  csv_file = 'sample.csv'\n",
        "  r=False\n",
        "  with open(csv_file, 'w', newline='') as csvfile:\n",
        "      csv_writer = csv.writer(csvfile)\n",
        "      for row_data in table_data:\n",
        "        if r:\n",
        "          csv_writer.writerow(row_data)\n",
        "        else:\n",
        "          r=True\n",
        "\n",
        "  print(f'Table data has been successfully written to {csv_file}.')"
      ],
      "metadata": {
        "id": "deFl5GEA9IfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_his_data(m,d,h,min):\n",
        "\n",
        "#------------------------GETTING HTML TABLE DATA---------------------------\n",
        "  url = f'https://ltp.investingdaddy.com/historical-option-chain.php?symbol=NIFTY&expiry=2023-09-07&filterdate1=2023-09-04&filtertime=09%3A15&filterdate=2023-{m}-{d}T{h}%3A{min}'\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "      html_source = response.text\n",
        "      #print(html_source)\n",
        "  else:\n",
        "      print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
        "\n",
        "  #------------------------FILTERING TABLE DATA-------------------------------\n",
        "  soup = BeautifulSoup(html_source, 'html.parser')\n",
        "  tables = soup.find_all('table')\n",
        "  ##price = soup.find('label', {'id':'future_val'})\n",
        "  ##price = price.text\n",
        "  ##prices.append(price)\n",
        "  ##return\n",
        "\n",
        "  if len(tables) >= 2:\n",
        "      second_table = tables[1]\n",
        "  else:\n",
        "      print(\"There are not enough tables in the HTML source.\")\n",
        "\n",
        "  #-----------------------CONVERTING HTML TABLE DATA TO CSV--------------------\n",
        "  html_content = \"<html>\"+str(second_table)+\"</html>\"\n",
        "  soup = BeautifulSoup(html_content, 'html.parser')\n",
        "  table = soup.find('table', {'id': 'tech-companies-1'})\n",
        "  table_data = []\n",
        "  rows = table.find_all('tr')\n",
        "  for row in rows:\n",
        "      row_data = []\n",
        "      cols = row.find_all(['th', 'td'])\n",
        "      for col in cols:\n",
        "          row_data.append(col.get_text(strip=True))\n",
        "      table_data.append(row_data)\n",
        "\n",
        "  csv_file = 'sample.csv'\n",
        "  r=False\n",
        "  with open(csv_file, 'w', newline='') as csvfile:\n",
        "      csv_writer = csv.writer(csvfile)\n",
        "      for row_data in table_data:\n",
        "        if r:\n",
        "          csv_writer.writerow(row_data)\n",
        "        else:\n",
        "          r=True\n",
        "\n",
        "  print(f'historical Table data has been successfully written to {csv_file}.')\n"
      ],
      "metadata": {
        "id": "lsq7Q2XIU25_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def changedatashape():\n",
        "\n",
        "  # Load your CSV data into a Pandas DataFrame\n",
        "  df = pd.read_csv('sample.csv')  # Replace 'your_data.csv' with your actual file path\n",
        "\n",
        "  # Check the shape of the original DataFrame (it should be 20x20)\n",
        "  #print(\"Original DataFrame Shape:\", df.shape)\n",
        "\n",
        "  # Convert the DataFrame into a NumPy array\n",
        "  data = df.to_numpy()\n",
        "\n",
        "  # Reshape the data into a 1D array and transpose it\n",
        "  horizontal_data = data.flatten().reshape(1, -1)\n",
        "\n",
        "  # Check the shape of the reshaped data (it should be 1x400)\n",
        "  #print(\"Horizontal Data Shape:\", horizontal_data.shape)\n",
        "\n",
        "  # If you want to save this reshaped data to a new CSV file:\n",
        "  horizontal_df = pd.DataFrame(horizontal_data)\n",
        "\n",
        "  # Save it to a new CSV file without row or column labels\n",
        "  horizontal_df.to_csv('sample2.csv', index=False, header=False)\n",
        "  os.remove('sample.csv')\n"
      ],
      "metadata": {
        "id": "UksXWc1Z9_mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(m,historical=True):\n",
        "  if historical:\n",
        "    date = input('date:')\n",
        "    hour = input('hour:')\n",
        "    min = input('min')\n",
        "    get_his_data(m,date,hour,min)\n",
        "  else:\n",
        "    getdata()\n",
        "  changedatashape()\n",
        "\n",
        "  # Specify the path to your CSV file\n",
        "  csv_file_path = 'sample2.csv'\n",
        "\n",
        "  # Read the existing data to determine the number of columns and rows\n",
        "  with open(csv_file_path, 'r') as csv_file:\n",
        "      reader = csv.reader(csv_file)\n",
        "      data = list(reader)\n",
        "      num_columns = len(data[0]) if data else 0\n",
        "      num_rows = len(data)\n",
        "\n",
        "  # Generate the header row for the new column\n",
        "  new_column_header = 'sample names'\n",
        "\n",
        "  # Generate the values for the new column\n",
        "  new_column_values = ['sample{}'.format(i-1) for i in range(1, num_rows + 1)]\n",
        "\n",
        "  # Insert the new column into the data\n",
        "  for i in range(num_rows):\n",
        "      data[i].insert(0, new_column_values[i])\n",
        "\n",
        "  # Write the modified data back to the file\n",
        "  with open(csv_file_path, 'w', newline='') as csv_file:\n",
        "      writer = csv.writer(csv_file)\n",
        "      writer.writerow([new_column_header] + ['feature{}'.format(i+1) for i in range(1, num_columns + 1)])\n",
        "      writer.writerows(data)\n"
      ],
      "metadata": {
        "id": "PsePN5y-_kPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(9,historical=True)\n",
        "#remember to change expiry every week\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlKFu1n4CtjK",
        "outputId": "315214f8-ddb4-450e-ec3a-7d4a3e5a2a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "date:05\n",
            "hour:15\n",
            "min11\n",
            "historical Table data has been successfully written to sample.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!java --enable-preview -jar /content/drive/MyDrive/trained_model/jadbio-model-exe.jar \\\n",
        "     -m /content/drive/MyDrive/trained_model/jadbio-1.4.119-model-Augustwithprice_feature0.bin \\\n",
        "     -i /content/sample2.csv \\\n",
        "     -o predictions.csv"
      ],
      "metadata": {
        "id": "-KeCPbIrQKbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predi = pd.read_csv('predictions.csv')\n",
        "if 0.5<predi.iloc[0,1]<0.63:\n",
        "  print(\"increase\")\n",
        "else:\n",
        "  print(\"decrease\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGH_SEUXFRss",
        "outputId": "15000af2-91a5-49a8-8115-35d7fccdcc3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "increase\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.remove('predictions.csv')\n",
        "os.remove('sample2.csv')"
      ],
      "metadata": {
        "id": "FjWaMx0_IETJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}